#### Steps:
1. **Create an S3 bucket**:
   - Go to the S3 dashboard in AWS.
   - Click **Create bucket**.
   - Choose a globally unique name and select a region.
   - Leave other settings as default, or adjust based on your needs.

2. **Set Public Access Control**:
   - Go to **Permissions**.
   - Under **Block public access (bucket settings)**, click **Edit**.
   - Disable all options except "Block new public ACLs and uploading public objects".
   - Click **Save changes**.

   **Warning**: Public access can be a security risk. Ensure that only the necessary files need to be publicly accessible, and use ACLs or policies to control access tightly.

3. **Bucket Policy (Optional)**: If specific objects should be publicly accessible (e.g., static website files), add a policy:
   ```json
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Sid": "PublicReadGetObject",
         "Effect": "Allow",
         "Principal": "*",
         "Action": "s3:GetObject",
         "Resource": "arn:aws:s3:::your-bucket-name/*"
       }
     ]
   }
   ```
   Replace `your-bucket-name` with your actual bucket name.

### 2. **Create IAM Roles with Minimal Access**

Minimal access follows the principle of least privilege. Only grant the permissions necessary for a given role.

#### Steps:
1. **Create IAM Role**:
   - Go to the **IAM Console**.
   - Click **Roles** > **Create role**.
   - Choose a trusted entity type (e.g., AWS service like EC2 or Lambda).
   - Attach policies that provide minimal permissions to access the S3 bucket.
   - For example, if the role needs access to S3 for uploading files:
     - Attach `AmazonS3FullAccess` (or better yet, a custom policy that only allows specific actions like `s3:PutObject` for a specific bucket).

2. **Define a Custom Policy (example)**:
   ```json
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Action": "s3:PutObject",
         "Resource": "arn:aws:s3:::your-bucket-name/*"
       }
     ]
   }
   ```
   This policy would only allow the action of uploading files (PutObject) to your S3 bucket.


### 3. **Automating File Uploads and Permissions Check**
# Install AWS CLI on Ubuntu
sudo apt-get install awscli
# If you are using ec2. You don't have to download.
# Install AWS CLI on macOS
brew install awscli
```

After installation, configure the AWS CLI with your access key and secret key:

```bash
aws configure
```

Enter your AWS access key, secret key, default region, and output format.

### Step 2: Automate File Upload to S3
To upload files to an S3 bucket, you can use the AWS CLI `aws s3 cp` command. Hereâ€™s how to write a Bash script to periodically upload files:

#### Bash Script for File Upload (upload_to_s3.sh)
1. Create a bash script file, e.g., `upload_to_s3.sh`.
2. Inside this script, use the `aws s3 cp` command to upload files to your S3 bucket.

```bash
#!/bin/bash

# Define local directory and S3 bucket
LOCAL_DIR="/home/ec2-user/newfolder" #use your own path, make sure you have files present.
S3_BUCKET="s3://mynewbucketzz/"

# Upload files to S3 (using --recursive for directories)
aws s3 cp "$LOCAL_DIR" "$S3_BUCKET" --recursive

# Output message
if [ $? -eq 0 ]; then
  echo "Files uploaded successfully to $S3_BUCKET"
else
  echo "File upload failed"
fi
```

#### Explanation:
- `LOCAL_DIR="/home/ec2-user/newfolder"`: Specifies the local directory where your files are located.
- `S3_BUCKET="s3://mynewbucketzz/"`: The target S3 bucket where you want to upload your files.
- `aws s3 cp "$LOCAL_DIR" "$S3_BUCKET" --recursive`: Copies all files and directories from the local folder to the S3 bucket.
- `$?`: Captures the exit status of the last command to check if the upload was successful.

### Step 3: Automate Permissions Check on S3 Bucket
You may also want to ensure that your S3 bucket's permissions are correct before or after uploading the files. You can do this by checking the bucket policy with the AWS CLI `aws s3api get-bucket-policy` command.

#### Bash Script for Permissions Check (check_permissions.sh)
1. Create a second script, `check_permissions.sh`, to check the current bucket policy.

```bash
#!/bin/bash

# Define S3 bucket
S3_BUCKET="your-bucket-name"

# Get the current bucket policy
aws s3api get-bucket-policy --bucket "$S3_BUCKET" > current_policy.json

# Check if the policy exists and is correct (this part is just an example)
if [ -s current_policy.json ]; then
  echo "Current bucket policy fetched successfully."
  # Optionally, you can process and check the policy here
else
  echo "Failed to retrieve the bucket policy or no policy set."
fi
```

#### Explanation:
- `aws s3api get-bucket-policy --bucket "$S3_BUCKET"`: Fetches the current bucket policy and outputs it to a file named `current_policy.json`.
- The script checks if the policy file is non-empty, indicating that a policy exists.

### Step 4: Automating the Scripts Using Cron Jobs
Now that you have the scripts ready, you can automate them using `cron` to periodically upload files and check permissions.

#### Set Up a Cron Job
1. Edit your cron table by running:

```bash
crontab -e
```

2. Add entries to run the scripts at your desired intervals. For example, to run the upload script every hour and the permissions check script every day:
# create upload.sh and create check_permissions.sh
```bash
# Run upload_to_s3.sh every hour
0 * * * * /home/ec2-user/upload.sh

# Run check_permissions.sh once a day at 2am
0 2 * * * /home/ec2-user/check_permissions.sh
```

#### Explanation:
- `0 * * * * /path/to/upload_to_s3.sh`: Runs the file upload script every hour.
- `0 2 * * * /path/to/check_permissions.sh`: Runs the permissions check script every day at 2 AM.

You can customize the cron schedule based on your needs.

### Step 5: Testing the Automation
To test the scripts:
1. Manually execute the scripts to ensure they work as expected.

```bash
bash /path/to/upload_to_s3.sh
bash /path/to/check_permissions.sh
```

2. Check the logs to verify that files are being uploaded to your S3 bucket, and that the permissions check is being conducted.

3. Ensure that cron jobs are running correctly.

### 4. **Enable GuardDuty to Monitor Alerts**

GuardDuty provides intelligent threat detection. Enabling it is straightforward.

#### Steps:
1. **Enable GuardDuty**:
   - Go to **GuardDuty** in the AWS console.
   - Click **Get started**.
   - Choose your region and enable the service.
   
2. **Configure GuardDuty Alerts**:
   - Once enabled, GuardDuty will automatically start monitoring.
   - You can create CloudWatch alarms to receive notifications for any findings.

3. **Integrate GuardDuty with AWS SNS for notifications**:
   - Go to **CloudWatch** > **Alarms** > **Create Alarm**.
   - Set the alarm to trigger on GuardDuty findings (you may need to create a custom CloudWatch event rule).
   - Choose SNS as the notification method to send alerts to a Slack channel, email, or other endpoints.

### 5. **Track Changes Using Git**

Using Git allows you to track changes to your configurations, scripts, and documentation.

#### Steps:
1. **Initialize a Git Repository**:
   - On your local machine, initialize a Git repository for your project:
     ```bash
     git init
     ```

2. **Commit Changes**:
   - Regularly commit your configuration files, scripts, and documentation:
     ```bash
     git add .
     git commit -m "Initial commit of S3 setup and IAM role configuration"
     ```

3. **Push to Remote Repository** (e.g., GitHub, GitLab):
   - Create a remote repository (GitHub, GitLab, etc.).
   - Push your changes:
     ```bash
     git remote add origin https://github.com/yourusername/your-repo.git
     git push -u origin master
     ```

### 6. **Document the Project**

A key part of a successful project is good documentation. This includes setup instructions, code comments, and explanations.

